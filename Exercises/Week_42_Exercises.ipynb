{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 42\n",
    "Group: Eyyüb Güven, Alicja Terelak, Giorgio\n",
    "Chirio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our own inversion:\n",
      "[[2.]\n",
      " [4.]\n",
      " [5.]]\n",
      "theta from our own grad descent:\n",
      "[[1.99679243]\n",
      " [4.00883833]\n",
      " [4.99576941]]\n",
      "theta from our own grad descent, using adagrad:\n",
      "[[4.25878806]\n",
      " [2.44746756]\n",
      " [3.42676722]]\n",
      "theta from our own grad descent, using adagrad and momentum of 0.3:\n",
      "[[3.84156197]\n",
      " [3.86583573]\n",
      " [4.1066359 ]]\n",
      "theta from SGD with AdaGrad and without momentum\n",
      "[[2.05027738]\n",
      " [3.87729596]\n",
      " [5.06184274]]\n",
      "theta from SGD with AdaGrad and with momentum\n",
      "[[1.99523586]\n",
      " [4.01246142]\n",
      " [4.99418317]]\n",
      "theta from RMSprop using autograd\n",
      "[[2.79829355]\n",
      " [2.37775333]\n",
      " [5.66509058]]\n",
      "theta from ADAM using autograd\n",
      "[[2.01684846]\n",
      " [4.00958899]\n",
      " [5.02237093]]\n"
     ]
    }
   ],
   "source": [
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(beta):\n",
    "    return (1.0/n)*np.sum((y-X @ beta)**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n=100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 2+(4*x)+(5*(x**2))\n",
    "\n",
    "\n",
    "X = np.c_[np.ones((n,1)),x, x**2]\n",
    "\n",
    "X_TX = X.T @ X\n",
    "\n",
    "linreg_theta = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "print(\"our own inversion:\")\n",
    "print(linreg_theta)\n",
    "\n",
    "# PLAIN GD\n",
    "\n",
    "H = (2/n) * X_TX\n",
    "EigVal, EigVect = np.linalg.eig(H)\n",
    "\n",
    "theta = np.random.randn(3,1)\n",
    "eta = 1.0/np.max(EigVal)\n",
    "Niterations = 1000\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "# Base Gradient Descent\n",
    "for interation in range(Niterations):\n",
    "    grads = training_gradient(theta)\n",
    "    theta -= eta*grads\n",
    "\n",
    "\n",
    "print(\"theta from our own grad descent:\")\n",
    "print(theta)\n",
    "\n",
    "\n",
    "# GD with AdaGrad, plain GD without momentum\n",
    "\n",
    "\n",
    "theta = np.random.randn(3,1)\n",
    "eta = 0.1\n",
    "Niterations = 1000\n",
    "training_gradient = grad(CostOLS)\n",
    "Giter = 0.0\n",
    "delta = 1e-8\n",
    "# Base Gradient Descent\n",
    "for interation in range(Niterations):\n",
    "    grads = training_gradient(theta)\n",
    "    Giter += grads*grads\n",
    "    update = grads*eta/(delta+np.sqrt(Giter))\n",
    "    theta -= update\n",
    "\n",
    "print(\"theta from our own grad descent, using adagrad:\")\n",
    "print(theta)\n",
    "\n",
    "# GD with AdaGrad, plain GD with momentum\n",
    "\n",
    "\n",
    "theta = np.random.randn(3,1)\n",
    "eta = 1.0/np.max(EigVal)\n",
    "Niterations = 1000\n",
    "training_gradient = grad(CostOLS)\n",
    "Giter = 0\n",
    "delta = 1e-8\n",
    "momentum = 0.3\n",
    "update = 0\n",
    "# Base Gradient Descent\n",
    "for interation in range(Niterations):\n",
    "    grads = training_gradient(theta)\n",
    "    Giter += grads*grads\n",
    "    new_update = grads*eta/(delta+np.sqrt(Giter)) + momentum * update\n",
    "    theta -= new_update\n",
    "    update = new_update\n",
    "\n",
    "print(\"theta from our own grad descent, using adagrad and momentum of 0.3:\")\n",
    "print(theta)\n",
    "\n",
    "\n",
    "\n",
    "# SGD with AdaGrad, without momentum\n",
    "\n",
    "def NewCostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "\n",
    "training_gradient = grad(NewCostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "# Value for learning rate\n",
    "eta = 1.0/np.max(EigVal)\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*eta/(delta+np.sqrt(Giter))\n",
    "        theta -= update\n",
    "print(\"theta from SGD with AdaGrad and without momentum\")\n",
    "print(theta)\n",
    "\n",
    "# SGD with AdaGrad, with momentum\n",
    "\n",
    "training_gradient = grad(NewCostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "# Value for learning rate\n",
    "eta = 1.0/np.max(EigVal)\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "momentum = 0.3\n",
    "old_update = 0\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*eta/(delta+np.sqrt(Giter)) + momentum * old_update\n",
    "        theta -= update\n",
    "        old_update = update\n",
    "print(\"theta from SGD with AdaGrad and with momentum\")\n",
    "print(theta)\n",
    "\n",
    "# RMS prop\n",
    "\n",
    "training_gradient = grad(NewCostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "# Value for learning rate\n",
    "eta = 1.0/np.max(EigVal)\n",
    "\n",
    "rho = 0.99\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        Giter += (rho*Giter+(1-rho)*gradients*gradients)\n",
    "        update = gradients*eta/(delta+np.sqrt(Giter))\n",
    "        theta -= update\n",
    "print(\"theta from RMSprop using autograd\")\n",
    "print(theta)\n",
    "\n",
    "# ADAM\n",
    "training_gradient = grad(NewCostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "# Value for learning rate\n",
    "eta = 1.0/np.max(EigVal)\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "iter = 0\n",
    "for epoch in range(n_epochs):\n",
    "    mom_1 = 0.0\n",
    "    mom_2 = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        mom_1 = beta1*mom_1 + (1-beta1)*gradients\n",
    "        mom_2 = beta2*mom_2 + (1-beta2)*gradients*gradients\n",
    "        term_1 = mom_1/(1.0-beta1**iter)\n",
    "        term_2 = mom_2/(1.0-beta2**iter)\n",
    "        update = eta*term_1/(np.sqrt(term_2)+delta)\n",
    "        theta -= update\n",
    "print(\"theta from ADAM using autograd\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
